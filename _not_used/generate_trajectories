#!/usr/bin/env python3
"""
Trajectory Generation Script for QuadPole2D Environment

This script generates observation, action, reward trajectories from the QuadPole2D environment
using either a trained stable-baselines3 policy or random actions.

Usage:
    python generate_trajectories.py --policy path/to/model.zip --episodes 100 --output trajectories.npz
    python generate_trajectories.py --policy random --episodes 50 --output random_trajectories.npz
"""

import argparse
import numpy as np
import pickle
from pathlib import Path
from typing import Dict, List, Tuple, Union
import warnings
from dataclasses import dataclass

try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    warnings.warn("tqdm not available. Progress bars will not be shown.")

# Import your environment (assuming it's in the same directory)
try:
    from quadrotor_env import QuadPole2D
except ImportError:
    # Try alternative import name
    try:
        from quadrotor_env_1 import QuadPole2D
    except ImportError:
        raise ImportError("Could not import QuadPole2D. Make sure the environment file is in the same directory.")

try:
    from stable_baselines3 import PPO, SAC, TD3, A2C, DQN
    from stable_baselines3.common.policies import BasePolicy
    SB3_AVAILABLE = True
except ImportError:
    SB3_AVAILABLE = False
    warnings.warn("stable-baselines3 not available. Only random policy will work.")


@dataclass
class TrajectoryData:
    """Container for trajectory data"""
    observations: List[np.ndarray]
    actions: List[np.ndarray]
    rewards: List[float]
    dones: List[bool]
    infos: List[Dict]
    episode_lengths: List[int]
    episode_returns: List[float]
    metadata: Dict


class TrajectoryGenerator:
    """Generates trajectories from QuadPole2D environment"""
    
    def __init__(self, config: Dict = None):
        """
        Initialize trajectory generator
        
        Args:
            config: Environment configuration dictionary
        """
        self.config = config or {
            'curriculum_level': 0,
            'pos_cost_multiplier': 1.0
        }
        
    def load_policy(self, policy_path: str) -> Union[BasePolicy, str]:
        """
        Load a stable-baselines3 policy or return 'random'
        
        Args:
            policy_path: Path to saved model or 'random'
            
        Returns:
            Loaded policy or 'random' string
        """
        if policy_path.lower() == 'random':
            return 'random'
            
        if not SB3_AVAILABLE:
            raise ImportError("stable-baselines3 is required to load trained policies")
            
        # Try to load with different SB3 algorithms
        algorithms = [PPO, SAC, TD3, A2C]
        
        for alg in algorithms:
            try:
                model = alg.load(policy_path)
                print(f"Successfully loaded {alg.__name__} model from {policy_path}")
                return model
            except Exception as e:
                continue
                
        raise ValueError(f"Could not load policy from {policy_path} with any supported algorithm")
    
    def generate_trajectories(
        self, 
        policy: Union[BasePolicy, str],
        num_episodes: int,
        max_steps_per_episode: int = 500,
        mode: str = 'test',
        manual_goal_position: Union[str, List, None] = None,
        verbose: bool = True
    ) -> TrajectoryData:
        """
        Generate trajectories using the given policy
        
        Args:
            policy: Trained policy or 'random'
            num_episodes: Number of episodes to generate
            max_steps_per_episode: Maximum steps per episode
            mode: Environment mode ('train' or 'test')
            manual_goal_position: Manual goal position or None for curriculum-based
            verbose: Whether to show progress bar
            
        Returns:
            TrajectoryData containing all trajectory information
        """
        # Initialize environment
        env = QuadPole2D(
            config=self.config,
            mode=mode,
            manual_goal_position=manual_goal_position,
            max_steps=max_steps_per_episode
        )
        
        # Storage for trajectory data
        all_observations = []
        all_actions = []
        all_rewards = []
        all_dones = []
        all_infos = []
        episode_lengths = []
        episode_returns = []
        
        # Generate episodes
        episode_iterator = range(num_episodes)
        if verbose and TQDM_AVAILABLE:
            episode_iterator = tqdm(episode_iterator, desc="Generating trajectories")
        elif verbose:
            print(f"Generating {num_episodes} episodes...")
            
        for episode in episode_iterator:
            obs, info = env.reset()
            episode_obs = [obs.copy()]
            episode_actions = []
            episode_rewards = []
            episode_dones = []
            episode_infos = [info.copy()]
            
            episode_return = 0.0
            step = 0
            
            while step < max_steps_per_episode:
                # Get action from policy
                if policy == 'random':
                    action = env.action_space.sample()
                else:
                    action, _ = policy.predict(obs, deterministic=False)
                    # Ensure action is numpy array
                    if not isinstance(action, np.ndarray):
                        action = np.array(action)
                
                # Take step in environment
                obs, reward, terminated, truncated, info = env.step(action)
                done = terminated or truncated
                
                # Store step data
                episode_obs.append(obs.copy())
                episode_actions.append(action.copy())
                episode_rewards.append(float(reward))  # Ensure it's a Python float
                episode_dones.append(bool(done))  # Ensure it's a Python bool
                episode_infos.append(info.copy())
                
                episode_return += reward
                step += 1
                
                if done:
                    break
            
            # Store episode data
            all_observations.extend(episode_obs)
            all_actions.extend(episode_actions)
            all_rewards.extend(episode_rewards)
            all_dones.extend(episode_dones)
            all_infos.extend(episode_infos)
            episode_lengths.append(len(episode_actions))
            episode_returns.append(float(episode_return))
            
            if verbose and episode % 10 == 0:
                avg_return = np.mean(episode_returns[-10:]) if len(episode_returns) >= 10 else np.mean(episode_returns)
                if TQDM_AVAILABLE and hasattr(episode_iterator, 'set_postfix'):
                    episode_iterator.set_postfix(avg_return=f"{avg_return:.2f}")
                else:
                    print(f"Episode {episode}, avg return (last 10): {avg_return:.2f}")
        
        # Create metadata
        metadata = {
            'num_episodes': num_episodes,
            'max_steps_per_episode': max_steps_per_episode,
            'environment_config': self.config,
            'environment_mode': mode,
            'manual_goal_position': manual_goal_position,
            'policy_type': 'random' if policy == 'random' else 'trained',
            'total_steps': len(all_actions),
            'avg_episode_length': np.mean(episode_lengths),
            'avg_episode_return': np.mean(episode_returns),
            'std_episode_return': np.std(episode_returns)
        }
        
        return TrajectoryData(
            observations=all_observations,
            actions=all_actions,
            rewards=all_rewards,
            dones=all_dones,
            infos=all_infos,
            episode_lengths=episode_lengths,
            episode_returns=episode_returns,
            metadata=metadata
        )
    
    def save_trajectories(self, trajectory_data: TrajectoryData, output_path: str):
        """
        Save trajectory data to file
        
        Args:
            trajectory_data: TrajectoryData object to save
            output_path: Output file path
        """
        output_path = Path(output_path)
        
        if output_path.suffix == '.npz':
            # Convert lists to numpy arrays for saving
            try:
                # Save as compressed numpy arrays
                np.savez_compressed(
                    output_path,
                    observations=np.array(trajectory_data.observations, dtype=np.float32),
                    actions=np.array(trajectory_data.actions, dtype=np.float32),
                    rewards=np.array(trajectory_data.rewards, dtype=np.float32),
                    dones=np.array(trajectory_data.dones, dtype=bool),
                    episode_lengths=np.array(trajectory_data.episode_lengths, dtype=int),
                    episode_returns=np.array(trajectory_data.episode_returns, dtype=np.float32),
                    # Save metadata and infos as pickled objects within npz
                    metadata=np.array([trajectory_data.metadata], dtype=object),
                    infos=np.array([trajectory_data.infos], dtype=object)
                )
            except Exception as e:
                print(f"Warning: Could not save as npz due to: {e}")
                print("Falling back to pickle format...")
                output_path = output_path.with_suffix('.pkl')
                with open(output_path, 'wb') as f:
                    pickle.dump(trajectory_data, f)
        elif output_path.suffix == '.pkl':
            # Save as pickle for full Python object preservation
            with open(output_path, 'wb') as f:
                pickle.dump(trajectory_data, f)
        else:
            raise ValueError(f"Unsupported file format: {output_path.suffix}")
            
        print(f"Trajectories saved to {output_path}")
        print(f"Total steps: {trajectory_data.metadata['total_steps']}")
        print(f"Average episode return: {trajectory_data.metadata['avg_episode_return']:.3f} ± {trajectory_data.metadata['std_episode_return']:.3f}")
    
    @staticmethod
    def load_trajectories(file_path: str) -> TrajectoryData:
        """
        Load trajectory data from file
        
        Args:
            file_path: Path to saved trajectory file
            
        Returns:
            TrajectoryData object
        """
        file_path = Path(file_path)
        
        if file_path.suffix == '.npz':
            data = np.load(file_path, allow_pickle=True)
            return TrajectoryData(
                observations=data['observations'].tolist(),
                actions=data['actions'].tolist(),
                rewards=data['rewards'].tolist(),
                dones=data['dones'].tolist(),
                infos=data['infos'].item(),  # Extract from object array
                episode_lengths=data['episode_lengths'].tolist(),
                episode_returns=data['episode_returns'].tolist(),
                metadata=data['metadata'].item()  # Extract from object array
            )
        elif file_path.suffix == '.pkl':
            with open(file_path, 'rb') as f:
                return pickle.load(f)
        else:
            raise ValueError(f"Unsupported file format: {file_path.suffix}")


def main():
    # ===== CONFIGURATION =====
    # Policy settings
    policy_path = "random"  # Change to path/to/your/model.zip for trained policy
    
    # Episode settings
    num_episodes = 100
    max_steps_per_episode = 500
    
    # Environment settings
    mode = "test"  # "train" or "test"
    curriculum_level = 0
    pos_cost_multiplier = 1.0
    
    # Goal position settings
    # Options: None (curriculum-based), [x, y] for fixed position, "dynamic-1" for changing goal
    manual_goal_position = None  # Example: [1.0, 1.0] or "dynamic-1"

    output_file = "trajectories.npz"  # Can be .npz or .pkl
    random_seed = 42
    verbose = True
    
    # ===== END CONFIGURATION SECTION =====
    
    print("=== QuadPole2D Trajectory Generation ===")
    print(f"Policy: {policy_path}")
    print(f"Episodes: {num_episodes}")
    print(f"Max steps per episode: {max_steps_per_episode}")
    print(f"Environment mode: {mode}")
    print(f"Curriculum level: {curriculum_level}")
    print(f"Goal position: {manual_goal_position}")
    print(f"Output file: {output_file}")
    print("=" * 40)
    
    # Set random seed
    if random_seed is not None:
        np.random.seed(random_seed)
        print(f"Set random seed to {random_seed}")
    
    # Parse goal position if it's a string
    if isinstance(manual_goal_position, str) and manual_goal_position != "dynamic-1":
        try:
            x, y = map(float, manual_goal_position.split(','))
            manual_goal_position = [x, y]
        except:
            raise ValueError(f"Invalid goal position format: {manual_goal_position}. Use [x,y] or 'dynamic-1'")
    
    # Create configuration
    config = {
        'curriculum_level': curriculum_level,
        'pos_cost_multiplier': pos_cost_multiplier
    }
    
    # Initialize generator
    generator = TrajectoryGenerator(config)
    
    # Load policy
    print(f"\nLoading policy: {policy_path}")
    try:
        policy = generator.load_policy(policy_path)
        print("Policy loaded successfully!")
    except Exception as e:
        print(f"Error loading policy: {e}")
        return
    
    # Generate trajectories
    print(f"\nGenerating {num_episodes} episodes...")
    try:
        trajectory_data = generator.generate_trajectories(
            policy=policy,
            num_episodes=num_episodes,
            max_steps_per_episode=max_steps_per_episode,
            mode=mode,
            manual_goal_position=manual_goal_position,
            verbose=verbose
        )
        print("Trajectory generation completed!")
    except Exception as e:
        print(f"Error generating trajectories: {e}")
        return
    
    # Save trajectories
    print(f"\nSaving trajectories to {output_file}...")
    try:
        generator.save_trajectories(trajectory_data, output_file)
        print("Successfully generated and saved trajectories!")
        
        # Print summary statistics
        print(f"\n=== Summary Statistics ===")
        print(f"Total steps collected: {len(trajectory_data.actions)}")
        print(f"Average episode length: {trajectory_data.metadata['avg_episode_length']:.1f}")
        print(f"Average episode return: {trajectory_data.metadata['avg_episode_return']:.3f}")
        print(f"Return std dev: {trajectory_data.metadata['std_episode_return']:.3f}")
        print(f"Min episode return: {min(trajectory_data.episode_returns):.3f}")
        print(f"Max episode return: {max(trajectory_data.episode_returns):.3f}")
        
    except Exception as e:
        print(f"Error saving trajectories: {e}")
        return

if __name__ == '__main__':
        main()
